# Content Discovery

# Robots.txt

The robots.txt file is a document that tells search engines which pages they are and aren't allowed to show on their search engine results or ban specific search engines from crawling the website altogether. It can be common practice to restrict certain website areas so they aren't displayed in search engine results. These pages may be areas such as administration portals or files meant for the website's customers. This file gives us a great list of locations on the website that the owners don't want us to discover as penetration testers.

# Manual discovery 
## Favicon
- The favicon is a small icon displayed in the browser's address bar or tab used for branding a website.

## sitemap.xml
- Unlike the robots.txt file, which restricts what search engine crawlers can look at, the sitemap.xml file gives a list of every file the website owner wishes to be listed on a search engine. These can sometimes contain areas of the website that are a bit more difficult to navigate to or even list some old webpages that the current site no longer uses but are still working behind the scenes.

# View Source code of page
- ```CTRL-U```

# Google Hacking/Dorking
<img width="918" height="642" alt="image" src="https://github.com/user-attachments/assets/7e3bff39-3646-4547-a43b-3eb3f6377fef" />
https://en.wikipedia.org/wiki/Google_hacking 

# Wappalyzer
- Wappalyzer (https://www.wappalyzer.com/) is an online tool and browser extension that helps identify what technologies a website uses, such as frameworks, Content Management Systems (CMS), payment processors and much more, and it can even find version numbers as well.

